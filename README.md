Group: Savithri Naik (20000054) and Dani Fallouh (10640663)
Github Repository link: https://github.com/Savithri02/ca_two_programming.git

Individual Report: Savithri Naik
Savithri Naik (20000054) and Dani Fallouh (10640663) did the coding together, as we both were interested in learning all the concepts of the project. We met on google meet as well as in the library for in person sessions and used to have shared screen sessions for the coding part. We started by brainstorming on what data or website we would be scraping to get the data. We explored many websites and finally we decided that we will go with the corona virus website as we found the data to be relatable and understandable.
We choose the website: https://www.worldometers.info/coronavirus/ as our source of data.  
Although the website had a lot of data, we chose country and continent wise data in the tabular form to be interesting as we knew that we could do a lot on analysis on such kind of data. We did the web scraping using beautiful soup library, chose certain columns of the data , saved each row into a dictionary and then saved each row as an object into MongoDB.
After getting the dataframe, we again divided the dataframes into country and continent wise then we pre-processed the data to make the data into numeric format. Based on the different dataframes, we created and answered different question to draw different conclusions on the data.
Summary: The project helped us learn many concepts such as different sources such as CSV files, Websites etc. , web scraping , saving and getting the data from MongoDB and using numpy pandas for  data analysis part. It was like a real-world example that we might have in the future as data analysts. In addition to it, we learnt how to make a GitHub repository, adding collaborators into it and push and pull changes into it .


Individual Report: Dani Fallouh

Savithri and I did all the coding together, as we both wanted to learn how to do web scraping. So we decided to get the data from this Website: https://www.worldometers.info/coronavirus/ 
This website has so much data, but we have only decided to scrab the data from the table which includes 9 columns for example: total cases, total deaths, active cases, total recovered etc. 
Using beautifulsoup library, we have saved each row as a dictionary: and then saved the row into MongoDB, each country or continent as an object.
Then we created a pandas data frame from the mongoDB site, and we have started the preprocessing part, by changing the values into numeric values, remove duplicates, make two different data frames (one for countries and one for continents). 
For the analysis part we have created and answered many questions in order to explain and understand the data more: we concentrated the most on doing graphs and charts to explain the data and find correlation in order to answer the real-life questions.
Summary: the project was very interesting because it is similar to real life exercises that we might have when we work as data scientists. We have learned how to do web scraping using beautiful soup, how to save the data into (and get data) mongodb, how to make a repository on github and push/pull changes, in addition to improving our python skills regarding data analysis.
